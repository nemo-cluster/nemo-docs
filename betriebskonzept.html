
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="de">
  <head>
    <meta charset="utf-8" />
    <title>Betriebskonzept &#8212; bwForCluster NEMO</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/translations.js"></script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <link rel="shortcut icon" href="_static/nemo-favicon.png"/>
    <link rel="index" title="Stichwortverzeichnis" href="genindex.html" />
    <link rel="search" title="Suche" href="search.html" />
    <link rel="next" title="Technisch Organisatorische Maßnahmen" href="toms.html" />
    <link rel="prev" title="bwForCluster NEMO" href="index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="https://cluster.nemo.uni-freiburg.de">
      
      <img src="_static/nemo-logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">bwForCluster NEMO</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Durchsuchen Sie die Dokumente ..." aria-label="Durchsuchen Sie die Dokumente ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <a href="https://cluster.nemo.uni-freiburg.de">bwForCluster NEMO</a>
        <a href="index.html">Dokumentationsindex</a>
        <p class="caption">
 <span class="caption-text">
  Dokumente
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Betriebskonzept
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="toms.html">
   Technisch Organisatorische Maßnahmen
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Navigation umschalten" aria-controls="site-navigation"
                title="Navigation umschalten" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Laden Sie diese Seite herunter"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/betriebskonzept.rst.txt"><button type="button"
                class="btn btn-secondary topbarbtn" title="Quelldatei herunterladen" data-toggle="tooltip"
                data-placement="left">.rst</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="In PDF drucken"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/nemo-cluster/nemo-docs"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Quell-Repository"><i
                    class="fab fa-github"></i>Repository</button></a>
        
        <a class="edit-button" href="https://github.com/nemo-cluster/nemo-docs/edit/main/docs/betriebskonzept.rst"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Bearbeite diese Seite"><i class="fas fa-pencil-alt"></i>vorschlagen zu bearbeiten</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Vollbildmodus"
        title="Vollbildmodus"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Inhalt
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#einleitung">
   Einleitung
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ziele-im-sinne-der-informationssicherheit">
   Ziele im Sinne der Informationssicherheit
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#erwartete-ergebnisse">
   Erwartete Ergebnisse
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dienstbeschreibung-hpc">
   Dienstbeschreibung HPC
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#betriebsmodell-bwforcluster-nemo">
   Betriebsmodell bwForCluster NEMO
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hardware-und-dienste">
     Hardware und Dienste
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ausgewahlte-dienste">
       Ausgewählte Dienste
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#ssh">
         SSH
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#scheduler">
         Scheduler
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#http-s">
         HTTP(S)
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#dnbd3">
         DNBD3
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#ansible">
         Ansible
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#openstack">
         OpenStack
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#dhcp">
         DHCP
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#monitoring">
         Monitoring
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deployment">
     Deployment
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#changemanagement">
     Changemanagement
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#updates-und-sicherheit">
     Updates und Sicherheit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parallel-und-home-speicher">
     Parallel- und HOME-Speicher
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#workspaces">
       Workspaces
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#netze">
     Netze
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#zugang-zur-ressource">
     Zugang zur Ressource
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kontingentierung">
     Kontingentierung
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#administration">
     Administration
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id26">
     Monitoring
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#verantwortlichkeiten">
   Verantwortlichkeiten
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#maschinensaal-ii-msii">
     Maschinensaal II (MSII)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#referenzen">
   Referenzen
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="betriebskonzept">
<h1>Betriebskonzept<a class="headerlink" href="#betriebskonzept" title="Link zu dieser Überschrift">¶</a></h1>
<table class="table">
<colgroup>
<col style="width: 68%" />
<col style="width: 32%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Gültig ab:</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Version:</p></td>
<td><p>0.4.3</p></td>
</tr>
<tr class="row-odd"><td><p>Datum:</p></td>
<td><p>25.05.2021</p></td>
</tr>
</tbody>
</table>
<div class="attention admonition">
<p class="admonition-title">TLP:AMBER</p>
<p>Limited disclosure, restricted to participants‘ organizations. Distribution outside this audience requires written permission from the originator.</p>
</div>
<table class="table">
<colgroup>
<col style="width: 4%" />
<col style="width: 4%" />
<col style="width: 6%" />
<col style="width: 85%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Version</p></th>
<th class="head"><p>Datum</p></th>
<th class="head"><p>Autor*innen</p></th>
<th class="head"><p>Änderungen</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0.4.3</p></td>
<td><p>25.05.2021</p></td>
<td><p>MJ,DvS</p></td>
<td><p>Kleinere Textkorrekturen. Testglossar zu Tooltipps geändert.</p></td>
</tr>
<tr class="row-odd"><td><p>0.4.2</p></td>
<td><p>21.05.2021</p></td>
<td><p>MJ</p></td>
<td><p>Weitere Korrekturen, die durch die Konversion von Latex zu rST notwendig wurden. Fußnoten nach Punkt. Testweise Glossar erstellt.</p></td>
</tr>
<tr class="row-even"><td><p>0.4.1</p></td>
<td><p>21.05.2021</p></td>
<td><p>JL</p></td>
<td><p>Kleinere Korrekturen</p></td>
</tr>
<tr class="row-odd"><td><p>0.4.0</p></td>
<td><p>20.04.2021</p></td>
<td><p>MJ</p></td>
<td><p>Einleitung gestrafft, theoretische Sicherheitszonen entfernt, zusätzliche Dokumente und Fußnoten, TOMs ausgelagert, Ziele und Ergebnisse nach vorne gezogen, neue Abschnitte zu Updates und Workspaces.</p></td>
</tr>
<tr class="row-even"><td><p>0.3.2</p></td>
<td><p>25.03.2021</p></td>
<td><p>MJ</p></td>
<td><p>Korrekturen, Ergänzungen.</p></td>
</tr>
<tr class="row-odd"><td><p>0.3.1</p></td>
<td><p>19.03.2021</p></td>
<td><p>DvS</p></td>
<td><p>Korrekturen, Ergänzungen.</p></td>
</tr>
<tr class="row-even"><td><p>0.3.0</p></td>
<td><p>13.03.2021</p></td>
<td><p>DvS</p></td>
<td><p>Umstrukturierung des Textes, stärkere Verknüpfung von Sicherheit und Betriebsmodell, Ergänzungen.</p></td>
</tr>
<tr class="row-odd"><td><p>0.2.1</p></td>
<td><p>09.03.2021</p></td>
<td><p>BW, JL, DvS, MJ</p></td>
<td><p>Anmerkungen und Korrekturen.</p></td>
</tr>
<tr class="row-even"><td><p>0.2.0</p></td>
<td><p>09.03.2021</p></td>
<td><p>DvS</p></td>
<td><p>Ergänzungen u.a. aus dem DFN-Paper zu Security-Domains und Absicherungskonzept.</p></td>
</tr>
<tr class="row-odd"><td><p>0.1.0</p></td>
<td><p>06.03.2021</p></td>
<td><p>MJ</p></td>
<td><p>Erster Entwurf basierend auf Informationen des Betriebskonzepts de.NBI-FR und einer Vorlage von TOMs aus Tübingen.</p></td>
</tr>
</tbody>
</table>
<div class="section" id="einleitung">
<h2>Einleitung<a class="headerlink" href="#einleitung" title="Link zu dieser Überschrift">¶</a></h2>
<p>Das Konzept, auf dessen Grundlage das bwForCluster NEMO betrieben wird,
geht auf Überlegungen in der Abteilung eScience des Rechenzentrums der
Universität Freiburg (RZ) zurück und wurde in verschiedenen Projekten
iterativ entwickelt. <a class="footnote-reference brackets" href="#id30" id="id1">1</a> Die für den bwForCluster NEMO beschaffte
Hardware – Rechenknoten, Speichersystem für wissenschaftliche Daten und
Infrastrukturserver – wird in Datenschränken untergebracht, die vom RZ
in hierfür dedizierten Räumlichkeiten betrieben werden. Die
Datenschränke werden vom RZ betreut und auf grundlegende
Betriebsbereitschaft (Strom, Kühlung) überwacht. Es werden primär zwei
große Speichersysteme genutzt, einerseits eine professionelle
Storage-Appliance für administrative und Nutzer*innendaten, andererseits
ein schnelles paralleles Dateisystem für die Nutzung durch Compute-Jobs.
Auf den Infrastrukturservern laufen diverse Dienste, die skriptgesteuert
mit Ansible-Rezepten installiert und konfiguriert werden. Die
Rechenknoten werden via netzwerkbasiertem Remote-Boot von dedizierten
Infrastrukturservern zustandslos betrieben. Auf diesen Knoten werden die
Jobs der Wissenschaftler*innen ausgeführt. Besonders der Anspruch, Daten
über längere Zeiträume hinweg in ihren Entstehungs- und
Verarbeitungskontexten reproduzierbar zu halten, führte zu einer
Komplementierung des traditionellen Betriebsmodells von HPC-Systemen um
virtuelle Maschinen und Container. In diesen Virtualisierten (VFU) und
Containerisieten (CFU) Forschungsumgebungen laufen die Anwendungen oder
Umgebungen, die von den Wissenschafter*innen kontrolliert
werden. <a class="footnote-reference brackets" href="#id31" id="id2">2</a> <a class="footnote-reference brackets" href="#id32" id="id3">3</a> <a class="footnote-reference brackets" href="#id33" id="id4">4</a></p>
<p>Innerhalb dieser Forschungsumgebungen haben Wissenschaftler*innen die
Freiheit, Forschungsdaten und deren Kontext so zu abstrahieren, dass sie
auf anderen Systemen weiter ausgeführt und reproduziert werden können.
Mit VFUs und CFUs werden Hardware, Software und Dienste so entkoppelt,
dass für den bwForCluster NEMO ein verbessertes Management von
Ressourcen erreicht wird. Ein Grundgedanke dieser konzeptionellen
Vorüberlegungen ist die Aufteilung in Schichten. Der hier vorgestellte
Dienst des bwForClusters NEMO deckt im Modell, das in <a class="footnote-reference brackets" href="#id34" id="id5">5</a> <a class="footnote-reference brackets" href="#id35" id="id6">6</a>
vorgestellt wurde, die Ebene der IT-Ressourcen ab. Für die
Betriebsorganisation wird sie innerhalb des bwForClusters NEMO weiter
differenziert in Schrank, Hardware und Dienste. Dieses Konzeptpapier
beschreibt den Betrieb des bwForClusters NEMO.</p>
</div>
<div class="section" id="ziele-im-sinne-der-informationssicherheit">
<h2>Ziele im Sinne der Informationssicherheit<a class="headerlink" href="#ziele-im-sinne-der-informationssicherheit" title="Link zu dieser Überschrift">¶</a></h2>
<p>Kernanliegen des bwForClusters NEMO ist, Forscher*innen den Zugang zu
Rechen- und Speicherkapazitäten zu geben. Als Schutzziel betrachtet, ist
dies die Sicherstellung der <em>Verfügbarkeit</em> von Rechenressourcen, auf
denen Forscher*innen ihre Rechenjobs ausführen. Als Dienstanbieter muss
das bwForCluster NEMO eine Verfügbarkeit bieten, die
Wissenschaftler*innen Big-Data-Analysen über das Maß selbst gemanagter
oder dezentral in Forschungseinrichtung administrierten IT-Cluster
hinaus ermöglicht.</p>
<p>Für die Forscher*innen ist, um ihre Datenverarbeitung und Ergebnisse
reproduzierbar zu halten, eine <em>Integrität</em> von Daten auf Speicherebene
notwendig. Für Integrität von Daten auf der von ihnen betriebenen
Verarbeitungsschicht sind sie selbst verantwortlich.</p>
<p>Das Schutzziel <em>Vertraulichkeit</em> bezieht sich auf die Kontrolle des
Zutritts, Zugangs und Zugriffs sowie die Trennung von Umgebungen. Mit der
Abstraktion der Dienstschichten – Konzentration auf der Ebene der
physischen und netztechnischen Infrastrukturen, der
Clusteradministration in HPC, der Entkoppelung von logischen
Nutzer-bezogenen Einheiten – sind Arbeitsteilungen möglich, die ein
größeres Spektrum an Diensten für die Forschung in den Bereich des
wirtschaftlich möglichen schieben. Sicherheitsrelevante Arbeitsbereiche
können zentral von qualifiziertem Personal abgedeckt werden.</p>
<p>Regulatorische Anforderungen an Forschung im Bereich der
Neurowissenschaft, die in einigen Bereichen personenbezogene
medizinische Daten mit hohem Schutzbedarf erzeugen, erfordern die
Sicherstellung, diese Daten innerhalb eines kontrollierbaren Rechtsraums
zu speichern. Die Souveränität wird auf die Kontrolle der physischen,
organisatorischen und operativen Aspekte bezogen.</p>
<p>Das bwForCluster NEMO ist mit diesen Überlegungen in der Lage, die aus
den strategischen Zielen abgeleiteten Schutzziele mit eigenen Maßnahmen
wirtschaftlich tragen und erreichen zu können. Das Dokument “Technisch
Organisatorische Maßnahmen – bwForCluster NEMO” geht auf die einzelnen
Schutzziele detaillierter ein.</p>
</div>
<div class="section" id="erwartete-ergebnisse">
<h2>Erwartete Ergebnisse<a class="headerlink" href="#erwartete-ergebnisse" title="Link zu dieser Überschrift">¶</a></h2>
<p>Mit dem Aufbau der Teildienste in technischen Schichten und der
flexiblen Boot-Prozedur können die strategischen Ziele des bwForClusters
NEMO mit den gegebenen Ressourcen erreicht und die
Informationssicherheit gewahrt werden. Die Gliederung der Schichten
erlaubt es, die Arbeitsbereiche zu trennen und die Risiken im Betrieb
einzelner Schichten besser zu isolieren. Besonders die im
Wissenschaftsbereich hohe Erwartung an Verfügbarkeit lässt sich besser
erreichen. Die Zahl der “Single Points of Failures” ist besser
kontrollierbar. Die Standardisierung in der Steuerung der Hardware
reduziert die Komplexität im Betrieb, die den Wissenschaftler*innen
gebotene Freiheit ist praktisch vollständig von der Betriebsschicht
getrennt. Die Abstraktion reduziert Angriffsvektoren auf die
Betriebsschicht, die durch Ereignisse auf der Ebene der
Wissenschaftler*innen eröffnet werden.</p>
</div>
<div class="section" id="dienstbeschreibung-hpc">
<h2>Dienstbeschreibung HPC<a class="headerlink" href="#dienstbeschreibung-hpc" title="Link zu dieser Überschrift">¶</a></h2>
<p>Es liegt eine Dienstbeschreibung für das HPC-Computing-Angebot des
Rechenzentrums im Rahmen des allgemeinen Servicekatalogs vor. Diese kann
online von den Seiten des Rechenzentrums abgerufen werden. <a class="footnote-reference brackets" href="#id36" id="id7">7</a> Diese
Dienstbeschreibung wird einem regelmäßigen Review-Prozess in der Runde
der Abteilungsleiter*innen unterzogen.</p>
</div>
<div class="section" id="betriebsmodell-bwforcluster-nemo">
<h2>Betriebsmodell bwForCluster NEMO<a class="headerlink" href="#betriebsmodell-bwforcluster-nemo" title="Link zu dieser Überschrift">¶</a></h2>
<p>Das Betriebsmodell beschreibt konkrete Schritte des Deployments und der
täglichen Produktion des HPC-Clusters. Hierzu wird eine Kombination aus
administrativen Infrastruktur (Server) und von den
Wisschenschaftler*inenn zu Berechnungen verwendeten Rechenknoten
eingesetzt.</p>
<div class="section" id="hardware-und-dienste">
<h3>Hardware und Dienste<a class="headerlink" href="#hardware-und-dienste" title="Link zu dieser Überschrift">¶</a></h3>
<p>Die installierte Hardware des bwForClusters NEMO besteht aus über 900
Rechenknoten und einigen dedizierten Servern für NEMO-Dienste. <a class="footnote-reference brackets" href="#id37" id="id8">8</a>
Virtuelle Maschinen als VFUs und Container (CFUs) werden ebenfalls auf
diesen Rechenknoten ausgeführt, wie reguläre Cluster-Jobs. Auf den
Rechenknoten (ausgenommen Knoten für interaktive Nutzung) werden immer
nur Jobs eines/einer Nutzers/Nutzerin ausgeführt. Zugang zum Cluster
erfolgt über sogenannte Login-Knoten,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">login1</span><span class="o">.</span><span class="n">nemo</span><span class="o">.</span><span class="n">uni</span><span class="o">-</span><span class="n">freiburg</span><span class="o">.</span><span class="n">de</span> <span class="p">(</span><span class="n">alias</span> <span class="n">login</span><span class="o">.</span><span class="n">nemo</span><span class="o">.</span><span class="n">uni</span><span class="o">-</span><span class="n">freiburg</span><span class="o">.</span><span class="n">de</span><span class="p">)</span>
<span class="n">login2</span><span class="o">.</span><span class="n">nemo</span><span class="o">.</span><span class="n">uni</span><span class="o">-</span><span class="n">freiburg</span><span class="o">.</span><span class="n">de</span>
</pre></div>
</div>
<p>den Visualisierungsknoten (Vis),</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vis1</span><span class="o">.</span><span class="n">nemo</span><span class="o">.</span><span class="n">uni</span><span class="o">-</span><span class="n">freiburg</span><span class="o">.</span><span class="n">de</span>
<span class="n">vis2</span><span class="o">.</span><span class="n">nemo</span><span class="o">.</span><span class="n">uni</span><span class="o">-</span><span class="n">freiburg</span><span class="o">.</span><span class="n">de</span>
</pre></div>
</div>
<p>und über das Openstack-Dashboard. Die Zugangsknoten sind im öffentlichen
Internet exponiert, welches jedoch auf das Belwü-Netz eingeschränkt
wurde. <a class="footnote-reference brackets" href="#id38" id="id9">9</a> Der Zugriff erfolgt primär über den SSH-Dienst. Beim
Openstack-Dashboard wird der Transport mit HTTPS abgesichert.</p>
<div class="section" id="ausgewahlte-dienste">
<h4>Ausgewählte Dienste<a class="headerlink" href="#ausgewahlte-dienste" title="Link zu dieser Überschrift">¶</a></h4>
<div class="section" id="ssh">
<h5>SSH<a class="headerlink" href="#ssh" title="Link zu dieser Überschrift">¶</a></h5>
<p>Dieser Dienst läuft auf allen Knoten und Servern. Mit ihm ist ein Login
von Wissenschaftler*innen und Administrator*innen über die Eingabe von Nutzername und Dienst-Passwort oder einen SSH-Key möglich.</p>
</div>
<div class="section" id="scheduler">
<h5>Scheduler<a class="headerlink" href="#scheduler" title="Link zu dieser Überschrift">¶</a></h5>
<p>Dieser Dienst ist auf dem Management-Server von NEMO aktiv und dient zum
“Scheduling” (Verteilen nach vorgegebenem Algorithmus) von Jobs auf dem
Cluster. Dazu sind auf den Rechenknoten Clients installiert, die Jobs
und Ressourcenverbrauch protokollieren und diese Information an den
Scheduler zurückmelden.</p>
</div>
<div class="section" id="http-s">
<h5>HTTP(S)<a class="headerlink" href="#http-s" title="Link zu dieser Überschrift">¶</a></h5>
<p>Das OpenStack-Dashboard ist als Webschnittstelle umgesetzt und setzt für
den Zugriff auf HTTPS, um eine Absicherung bei der Nutzung über das
öffentliche Belwü-Netz zu erreichen. Der Zugang erfolgt über Nutzername
und Dienst-Passwort. Auf dem Deployment-Server wird HTTP verwendet, um
Konfigurationen zu den Rechenknoten zu verteilen (Teil des
iPXE-basierten Boot-Ablaufs und der individuellen Knotenkonfiguration).
Die Deployment-Server sind nur im internen NEMO-Netz erreichbar.</p>
</div>
<div class="section" id="dnbd3">
<h5>DNBD3<a class="headerlink" href="#dnbd3" title="Link zu dieser Überschrift">¶</a></h5>
<p>Auf den Deployment-Servern laufen zwei
Distributed-Network-Block-Device-3-Instanzen. Dieser Dienst stellt das
Betriebssystem für Login-, Vis- und Rechenknoten zur Verfügung. Eine
redundante Auslegung stellt sicher, dass bei Ausfall eine Servers das Cluster weiterhin mit dem Betriebssystem-Image versorgt
wird.</p>
</div>
<div class="section" id="ansible">
<h5>Ansible<a class="headerlink" href="#ansible" title="Link zu dieser Überschrift">¶</a></h5>
<p>Auf dem Management-Server übernimmt Ansible das Ausrollen der Dienste
und deren Konfiguration.</p>
</div>
<div class="section" id="openstack">
<h5>OpenStack<a class="headerlink" href="#openstack" title="Link zu dieser Überschrift">¶</a></h5>
<p>Mehrere Openstack-Server und -Dienste sind Cluster-intern für die
Nutzung von VFUs zuständig.</p>
</div>
<div class="section" id="dhcp">
<h5>DHCP<a class="headerlink" href="#dhcp" title="Link zu dieser Überschrift">¶</a></h5>
<p>Die IP-Adressen werden bei Rechen-, Login-, sowie Visualisierungsknoten
über DHCP verteilt. Dieser Dienst wird von der Abteilung “Netze und
Kommunikationsdienste” mithilfe der Appliance Infoblox betrieben. <a class="footnote-reference brackets" href="#id39" id="id10">10</a></p>
</div>
<div class="section" id="monitoring">
<h5>Monitoring<a class="headerlink" href="#monitoring" title="Link zu dieser Überschrift">¶</a></h5>
<p>Der Monitoring-Server empfängt und speichert alle Log- und
Protokoll-Dateien. Hierbei werden Login-Versuche, kritische Fehler und
Hardware-Parameter protokolliert und teilweise visualisiert. Für
einfache Parameter wie die Temperatur eines Knotens sind Grenzwerte
definiert. Bei Überschreitung dieser werden die Administrator*innen des
Clusters per Mail verständigt.</p>
</div>
</div>
</div>
<div class="section" id="deployment">
<h3>Deployment<a class="headerlink" href="#deployment" title="Link zu dieser Überschrift">¶</a></h3>
<p>Die Dienste beim bwForCluster NEMO werden über Ansible-Rollen auf den
Serverknoten aufgesetzt. Das ermöglicht ein schnelles und einfaches
Ausrollen auf neuen Servern. Es müssen nur wenige Anpassungen
durchgeführt werden.</p>
<p>Das Boot- und Betriebssystem der Rechenknoten wird ebenfalls über Ansible generiert. Hierzu wird
das CentOS-Vorlagen-Image mit Ansible konfiguriert und in in ein
lesbares QCOW2-Image konvertiert. <a class="footnote-reference brackets" href="#id40" id="id11">11</a> Mit dem in der Abteilung
“eScience” entwickelten Boot-Framework wird dann das Image über das
Netzwerk gestartet. Das Image wird dabei über das nur lesbare
Blockdevice DNBD3 eingebunden. Für Schreiboperationen wird eine
Copy-on-write-Schicht darüber gelegt, die bei jedem Boot eines Knotens
frisch initialisiert wird. Alle neu generierten Images bekommen eine
inkrementierte Revisionsnummer, so dass die Umgebung zum einen
reproduzierbar ist, zum anderen bei Problemen mit einer Revision einfach
auf eine ältere zurück gegriffen werden kann.</p>
<p>Die Entscheidung, welche Systemversion, Revision und Konfiguation
geladen wird, trifft der sogenannte Bootauswahlserver anhand der
Zugehörigkeit der MAC-Adresse der Netzwerkkarte, über die der initiale
Start lief, zu einer Boot-Gruppe. <a class="footnote-reference brackets" href="#id41" id="id12">12</a> Diese Information wird jedesmal
beim Boot ausgewertet. Die Boot-Gruppe entscheidet über die
Konfiguration des Knotens. Sie wird verwendet, um spezielle Knoten zu
konfigurieren, beispielsweise bei GPU-Knoten. Bei neuer Hardware durch
Neubeschaffungen oder Ersatz bei Reparaturen muss lediglich die
MAC-Adresse einer Gruppe zugeordnet werden. Neue Konfigurationen können
ebenfalls schnell eingerichtet werden, da nur die zur Basisgruppe
unterschiedliche Konfiguration vorgenommen werden muss.</p>
</div>
<div class="section" id="changemanagement">
<h3>Changemanagement<a class="headerlink" href="#changemanagement" title="Link zu dieser Überschrift">¶</a></h3>
<p>Der Deploymentprozess erleichtert das Changemanagement. Die
Bereitstellung des Basissystems erlaubt schnelle Funktionstests, da beim
Netzwerk-Boot lediglich die neuere Version angefahren werden muss. Die
Hardwaregrundlage der Rechenknoten verändert sich im Laufe der
Beschaffungszyklen, jedoch wird im Beschaffungsprozess und beim Design
des Basissystems darauf geachtet, dass neue Knoten ohne Brüche in das
Grundsystem übernommen werden können. Die Heterogenität wird durch den
kontinuierlichen Austausch von Hardware verursacht, für die jeweils die
zum Moment der Beschaffung günstigsten oder passendsten Komponenten
verwendet werden.</p>
<p>Für jede Geräteklasse wird ein Knoten reserviert, mit dem ausschließlich
Tests durchgeführt werden. Erst wenn bei Änderungen am Grundsystem oder
Patches auf den reservierten Knoten durchgetestet wurden, werden diese
Änderungen auf den produktiven Knoten ausgerollt.</p>
</div>
<div class="section" id="updates-und-sicherheit">
<h3>Updates und Sicherheit<a class="headerlink" href="#updates-und-sicherheit" title="Link zu dieser Überschrift">¶</a></h3>
<p>Bei allen Servern, die keinen direkten Zugriff durch die
Wissenschaftler*innen erlauben, werden Updates bei den größeren
Wartungen eingespielt, die üblicherweise ein bis zwei Mal im Jahr statt
finden. Sollte eine außerordentliche Sicherheitslücke bestimmte Dienste
betreffen, wird das Update sobald es verfügbar ist, eingespielt. Sollte
hierzu ein Herunterfahren des Clusters notwendig werden, kann sich das
Update um bis zu vier Tage verzögern. Das Vorgehen wird dann im
eScience-Team unter Zuhilfenahme zusätzlicher IT-Experten diskutiert.
Diese Wartungen werden an die Wissenschaftler*innen vorab kommuniziert.</p>
<p>Bei den Login-, Vis- und Rechenknoten werden monatliche Updates
eingespielt. Dabei findet ein Rolling-Update statt. Das Cluster wird
offline genommen und neue Jobs können erst wieder starten, wenn die
Rechenknoten mit der neuen Systemversion gebootet sind. Damit können
alte Jobs noch zu Ende laufen, neue Jobs jedoch nur noch in der neuen
Umgebung starten. Durch das <a class="reference internal" href="#deployment">Deployment</a> und <a class="reference internal" href="#changemanagement">Changemanagement</a> kann
bei Problemen auf eine ältere Version gewechselt werden. Bei
außerordentlichen Sicherheitslücken wird das Update, sobald es verfügbar
ist, eingespielt und ausgerollt. Durch dieses Rolling-Update sind die
Patches bei allen Knoten eingespielt, wenn der Job, der zum Zeitpunkt
des Ausrollens noch die längste Restlaufzeit besitzt, endet und die vom
Job verwendeten Knoten neu booten können. Da die derzeitige maximale
Laufzeit der Jobs vier Tage beträgt, ist ein reguläres Update spätestens
nach vier Tagen beendet.</p>
</div>
<div class="section" id="parallel-und-home-speicher">
<h3>Parallel- und HOME-Speicher<a class="headerlink" href="#parallel-und-home-speicher" title="Link zu dieser Überschrift">¶</a></h3>
<p>Die HOME-Verzeichnisse der Nutzer*innen liegen auf dem Isilon-Speicher
der Universität. <a class="footnote-reference brackets" href="#id42" id="id13">13</a> Für die aktuell verarbeiteten wissenschaftlichen
Daten dient ein zentraler Parallelspeicher, der auf BeeGFS
aufsetzt. <a class="footnote-reference brackets" href="#id43" id="id14">14</a> Anders als der Isilon-Speicher ist der parallele
Speicher nur durch ein RAID6 abgesichert und bietet keine weiteren
Backups. Auf diesem Speicher sollten nur Daten liegen, die unmittelbar
für Berechnungen benötigt werden. Für eine anschließende Speicherung der
auf dem Cluster nicht mehr benötigten Daten wird bis Ende 2021 eine
Lösung auf dem bwSFS angeboten. <a class="footnote-reference brackets" href="#id44" id="id15">15</a></p>
<p>Der Parallelspeicher ist neben dem bwForCluster NEMO ebenfalls in der
ATLAS-Umgebung eingebunden. Diese beinhaltet das ATLAS-Cluster und die
ATLAS-VFU. <a class="footnote-reference brackets" href="#id45" id="id16">16</a> Dadurch können zusätzlich Nutzer*innen und
Administrator*innen der Freiburger ATLAS-Gruppen auf diesen Speicher
zugreifen.</p>
<p>Nutzer*innen können in der Standardeinstellung nur ihre eigenen Daten
einsehen und bearbeiten. Administrator*innen können alle Daten, sofern
sie nicht Nutzer- oder Client-seitig verschlüsselt wurden, einsehen und
bearbeiten. Beide Speicher werden nicht standardmäßig verschlüsselt.</p>
<div class="section" id="workspaces">
<h4>Workspaces<a class="headerlink" href="#workspaces" title="Link zu dieser Überschrift">¶</a></h4>
<p>Die Daten, die auf dem parallelen Speicher liegen, werden für die
Berechnungen der Wissenschaftler*innen benötigt. Das Management der
Daten wird durch die Forscher*innen in sogenannten “Workspaces”
durchgeführt. <a class="footnote-reference brackets" href="#id46" id="id17">17</a> Die Nutzer*innen müssen Workspaces anlegen, um den
parallelen Speicher verwenden zu können. Dabei kann ein Workspace
maximal 100 Tage gültig sein. Es besteht jedoch die Möglichkeit, jeden Workspace
99 mal 100 Tage zu verlängern. Die Wissenschaftler*innen werden 7 Tage vor Ablauf eines
Workspaces per Mail informiert.</p>
<p>Es wird empfohlen, für unterschiedliche Unterprojekte und separate
Berechnungen eigene Workspaces anzulegen. Jeder Workspace kann damit in
einem späteren Schritt als separate Einheit oder Objekt mit Metadaten
versehen in einem Wissenschaftsspeicher wie bwSFS gesichert werden.
Sinnvolle Einheiten/Workspaces müssen durch die Wissenschaftler*innen
selbst definiert werden.</p>
</div>
</div>
<div class="section" id="netze">
<h3>Netze<a class="headerlink" href="#netze" title="Link zu dieser Überschrift">¶</a></h3>
<p>Die Netzwerkanbindung der Serverschränke im Maschinensaal und der
zentralen Switche wird von der Abteilung “eScience” in Zusammenarbeit
mit der Abteilung “Netze und Kommunikationsdienste” (Netzwerkabteilung)
im RZ durchgeführt. Diese Anbindung erlaubt eine Administration der
Knoten in den Schränken von festgelegten IP-Adressen aus, die nur in
Räumen der Universität Freiburg sowie über VPN-Verbindungen zugewiesen
werden.</p>
<p>Die internen Uni-Netzwerke für das bwForCluster NEMO, die VFUs, das
ATLAS-Cluster und die Isilon sind voneinander getrennt und lassen nur
Zugriff von zum Betrieb notwendigen Netzen zu. Welche dies im einzelnen
sind, müssen vom jeweiligen Dienst erfragt werden.</p>
<p>Das bwForCluster NEMO verwendet folgende Netze:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">10.16</span><span class="o">.</span><span class="mf">0.0</span><span class="o">/</span><span class="mi">16</span>          <span class="n">NEMO</span><span class="p">:</span> <span class="n">Rechenknoten</span><span class="p">,</span> <span class="n">Server</span> <span class="n">und</span> <span class="n">Parallelspeicher</span>
                            <span class="n">Login</span><span class="o">-</span> <span class="n">und</span> <span class="n">Vis</span><span class="o">-</span><span class="n">Knoten</span> <span class="n">über</span> <span class="n">interne</span> <span class="n">Netzwerkschnittstelle</span>
<span class="mf">132.230</span><span class="o">.</span><span class="mf">222.0</span><span class="o">/</span><span class="mi">24</span>      <span class="n">NEMO</span><span class="p">:</span> <span class="n">Login</span><span class="o">-</span> <span class="n">und</span> <span class="n">Visualisierungsknoten</span>
<span class="mf">10.17</span><span class="o">.</span><span class="mf">0.0</span><span class="o">/</span><span class="mi">16</span>          <span class="n">NEMO</span><span class="p">:</span> <span class="n">CMS</span><span class="o">-</span><span class="n">VFU</span>
<span class="mf">10.18</span><span class="o">.</span><span class="mf">0.0</span><span class="o">/</span><span class="mi">16</span>          <span class="n">NEMO</span><span class="p">:</span> <span class="n">ATLAS</span><span class="o">-</span><span class="n">VFU</span>
<span class="mf">10.20</span><span class="o">.</span><span class="mf">0.0</span><span class="o">/</span><span class="mi">21</span>          <span class="n">NEMO</span><span class="p">:</span> <span class="n">NEMO</span><span class="o">-</span><span class="n">VFU</span> <span class="p">(</span><span class="n">unused</span><span class="p">)</span>
<span class="mf">10.20</span><span class="o">.</span><span class="mf">8.0</span><span class="o">/</span><span class="mi">21</span>          <span class="n">NEMO</span><span class="p">:</span> <span class="n">NEMO</span><span class="o">-</span><span class="n">VFU</span> <span class="p">(</span><span class="n">unused</span><span class="p">)</span>
<span class="mf">10.20</span><span class="o">.</span><span class="mf">16.0</span><span class="o">/</span><span class="mi">21</span>         <span class="n">NEMO</span><span class="p">:</span> <span class="n">NEMO</span><span class="o">-</span><span class="n">VFU</span> <span class="p">(</span><span class="n">unused</span><span class="p">)</span>
<span class="mf">10.20</span><span class="o">.</span><span class="mf">24.0</span><span class="o">/</span><span class="mi">21</span>         <span class="n">NEMO</span><span class="p">:</span> <span class="n">NEMO</span><span class="o">-</span><span class="n">VFU</span> <span class="p">(</span><span class="n">unused</span><span class="p">)</span>
<span class="mf">10.20</span><span class="o">.</span><span class="mf">32.0</span><span class="o">/</span><span class="mi">21</span>         <span class="n">NEMO</span><span class="p">:</span> <span class="n">NEMO</span><span class="o">-</span><span class="n">VFU</span> <span class="p">(</span><span class="n">unused</span><span class="p">)</span>
<span class="mf">10.20</span><span class="o">.</span><span class="mf">40.0</span><span class="o">/</span><span class="mi">21</span>         <span class="n">NEMO</span><span class="p">:</span> <span class="n">ATLAS</span><span class="o">-</span><span class="n">TEST</span><span class="o">-</span><span class="n">VFU</span>
</pre></div>
</div>
<p>Obige Netze sind jeweils voneinander getrennt. Lediglich die ATLAS-VFU
und ATLAS-TEST-VFU können zusätzlich auf das NEMO-Netz <code class="docutils literal notranslate"><span class="pre">10.16.0.0/16</span></code>
zugreifen. Das Cluster kann ansonsten nur über die öffentliche
IP-Adressen der Login- und Vis-Knoten erreicht werden. Die Rechenknoten
sind mit mindestens 1 <abbr title="Gigabit Ethernet">GbE</abbr> versorgt. Server, die Dienste anbieten, sind mit
mindestens zwei Anschlüssen mit 10 <abbr title="Gigabit Ethernet">GbE</abbr> über das Link Aggregation Control
Protocol (LACP) an zwei Top-Level-Switche angebunden. <a class="footnote-reference brackets" href="#id47" id="id18">18</a> Zusätzlich
sind alle Rechenknoten mit dem Hochgeschwindigkeitsnetzwerk “Omni-Path”
mit 100 <abbr title="Gigabit pro Sekunde">Gbit/s</abbr> untereinander und dem wissenschaftlichen Parallelspeicher
verbunden. <a class="footnote-reference brackets" href="#id48" id="id19">19</a></p>
</div>
<div class="section" id="zugang-zur-ressource">
<h3>Zugang zur Ressource<a class="headerlink" href="#zugang-zur-ressource" title="Link zu dieser Überschrift">¶</a></h3>
<p>Zugang zum bwForCluster NEMO haben lediglich registrierte
Forscher*innen. Antragsberechtigt sind nur Wissenschaftler*innen aus
Baden-Württemberg. Die genauen Zugangskriterien und die einzelnen
Schritte der Registrierungsprozedur sind im bwHPC-Wiki
beschreiben. <a class="footnote-reference brackets" href="#id49" id="id20">20</a> Für das bwForCluster NEMO muss von dem/der
Wissenschaftler*in ein separates Dienst-Passwort angelegt werden.</p>
<p>Das Auslaufen und die Invalidierung von Accounts regelt jede Universität
selbst. Der Nutzer hat danach keinen Zugriff mehr auf die Ressourcen.
Die Daten der Nutzer*innen verbleiben jedoch so lange auf dem Cluster,
bis die Ressource abgeschaltet wird oder die Anfrage einer berechtigten
Person erfolgt. Es gibt derzeit keine festen Regeln diesbezüglich, so
dass diese Frage einer genaueren Ausarbeitung Bedarf. Für das
Nachfolgecluster, das voraussichtlich im Jahr 2022 in Betrieb gehen
wird, wird eine Lösung erarbeitet. Die Universität stellt hierzu die
folgenden Ordnungen zur Verfügung:</p>
<ul class="simple">
<li><p>Verwaltungs- und Benutzungsordnung (VBO). <a class="footnote-reference brackets" href="#id50" id="id21">21</a></p></li>
<li><p>Benutzungsordnung für die vom Rechenzentrum der Albert-Ludwigs-Universität angebotenen Netzdienste: (NBO). <a class="footnote-reference brackets" href="#id51" id="id22">22</a></p></li>
<li><p>Netzordnung für das Freiburger Universitäts Netz: (NO). <a class="footnote-reference brackets" href="#id52" id="id23">23</a></p></li>
</ul>
</div>
<div class="section" id="kontingentierung">
<h3>Kontingentierung<a class="headerlink" href="#kontingentierung" title="Link zu dieser Überschrift">¶</a></h3>
<p>Die Wissenschaftler*innen sind im Sinne der gemeinschaftlichen
DFG-Beantragung Stakeholder des bwForClusters NEMO. Zusätzlich gibt es
Shareholder, die mit eigenen Mitteln Teile das Clusters mitfinanziert
haben. <a class="footnote-reference brackets" href="#id53" id="id24">24</a> Diesen stehen zusätzliche Anteile am Cluster zur Verfügung.
Die Regelung, wer wie viele Ressourcen des Clusters nutzen kann, wird
über einen “Fairshare-Mechanismus” geregelt. <a class="footnote-reference brackets" href="#id54" id="id25">25</a> Dieser bestimmt, wann
ein Job eines/r Wissenschaftlers/in starten kann. Hierzu wird von einer
Gruppe jeweils der Verbrauch der letzten drei Monate mit ihrem “Share”
verglichen. Ist der Verbrauch höher als der Share, der der Arbeitsgruppe
zur Verfügung steht, werden die Jobs niedriger priorisiert, ist er
niedriger als der verfügbare Share, werden die Jobs höher priorisiert.
Wissenschaftler*innen können aber mehr Ressourcen verwenden, als ihnen
aufgrund ihres Shares zustehen würden. Sie werden dadurch in Zukunft nur
schlechter in der Warteschlange priorisiert. Es gibt lediglich eine
maximale Anzahl an Ressourcen, die ein/e Wissenschaftler*in gleichzeitig
in die Warteschlange stellen kann.</p>
</div>
<div class="section" id="administration">
<h3>Administration<a class="headerlink" href="#administration" title="Link zu dieser Überschrift">¶</a></h3>
<p>Administrator*innen verfügen über erweiterte Rechte. Sie haben Zugriff
auf alle Daten der Nutzer*innen, sofern diese nicht zusätzlich
verschlüsselt werden. Der administrative Zugang wird bei Bedarf manuell
gewährt und wird bei Ausscheiden, beziehungsweise wenn die Rechte nicht
mehr benötigt werden, manuell entzogen. Derzeit wird ein Protokoll für
die Administration entwickelt, das diesen Aspekt regelt. Die Einführung
des Protokolls zum Ein- beziehungsweise Austritt von Administrator*innen
ist für den Start des bwForClusters NEMO2 2022 geplant.</p>
</div>
<div class="section" id="id26">
<h3>Monitoring<a class="headerlink" href="#id26" title="Link zu dieser Überschrift">¶</a></h3>
<p>Das Monitoring überwacht den dauerhaften Betrieb mit Verfolgung der
Ziele Verfügbarkeit, Vertraulichkeit und Integrität der Daten. Beim
Monitoring werden Schränke, Infrastrukturkomponenten wie Netzwerk,
Speichersysteme, Server und Rechenknoten überwacht. Neben der
Überwachung der Hardware wird die Temperatur, Stromaufnahme und
zusätzlich bei Schränken die Luftfeuchtigkeit kontrolliert. Die
Nachverfolgung des Netzwerks findet in der Netzwerkabteilung und bei
Schränken in der Abteilung “Allgemeiner Betrieb” statt. Strom und
Kühlung werden zudem vom “Technischen Gebäugemanagement” (TGM)
überwacht. Zusätzlich protokolliert der Monitoring-Server des Clusters
mit Hilfe von Zabbix Hardwaredaten wie Temperatur und Defekte auf
Knotenebene und schlägt beim Überschreiten von Grenzwerten per Mail
Alarm. <a class="footnote-reference brackets" href="#id55" id="id27">26</a> Zabbix überprüft laufend, ob die Dienste, die auf den
Servern laufen müssen, noch aktiv sind. Es wird allerdings nicht
geprüft, ob die Dienste noch korrekt funktionieren.</p>
<p>Außerdem werden Hardware- sowie Softwareprobleme, Login- und
Zugriffsversuche über <code class="docutils literal notranslate"><span class="pre">rsyslog</span></code> lokal auf der SSD und für die von den
Wissenschaftler*innen erreichbaren Knoten wie Login-, Vis- und
Rechenknoten zusätzlich auf dem Monitoringserver in Dateien gespeichert.</p>
<p>Der Speicherverbrauch im parallelen Dateisystem und den
Home-Verzeichnissen wird mittels Quotas auf Nutzerebene durchgesetzt.
Die Auslastung wird jeweils von den zuständigen Betreibern ermittelt.
Bei Isilon ist das die Abteilung “Virtualisierung und Speichersysteme”,
beim BeeGFS machen das die Administrator*innen des bwForClusters NEMO.
“Workspaces” auf dem parallelen Wissenschaftsspeicher BeeGFS haben eine
Laufzeit von 100 Tagen und müssen von den Wissenschaftler*innen mit einem Kommando
manuell verlängert werden. Erfolgt das nicht, werden die Daten endgültig
nach einer Wartezeit von sieben Tagen gelöscht.</p>
</div>
</div>
<div class="section" id="verantwortlichkeiten">
<h2>Verantwortlichkeiten<a class="headerlink" href="#verantwortlichkeiten" title="Link zu dieser Überschrift">¶</a></h2>
<p>Die Verantwortung für den Betrieb des bwForClusters NEMO liegt bei
dem/der Leiter*in der Abteilung eScience. Diese/r berichtet der/dem
Leiter*in des Rechenzentrums der Universität Freiburg.</p>
<div class="section" id="maschinensaal-ii-msii">
<h3>Maschinensaal II (MSII)<a class="headerlink" href="#maschinensaal-ii-msii" title="Link zu dieser Überschrift">¶</a></h3>
<p>Der MSII sowie die darüber bereitgestellten Schränke werden von der
Abteilung “Allgemeiner Betrieb” verantwortet. Das operative Geschäft
sowie die organisatorischen Schnittstellen innerhalb des RZ sowie zu
Nutzer*innen, die Ressourcen im Maschinensaal betreiben, werden in der
“Maschinensaalbenutzungsordnung” <a class="footnote-reference brackets" href="#id56" id="id28">27</a> für den Maschinensaal
beschrieben. Die Nutzung der Server-Schränke wird im Dienstkatalog
“Machine-Hosting” <a class="footnote-reference brackets" href="#id57" id="id29">28</a> spezifiziert. Die Maschinensaalbenutzungsordnung
bestimmt ebenfalls den physikalischen Zugriff der Administrator*innen
des Clusters auf die Schränke und die darin eingebauten Maschinen.</p>
</div>
</div>
<div class="section" id="referenzen">
<h2>Referenzen<a class="headerlink" href="#referenzen" title="Link zu dieser Überschrift">¶</a></h2>
<dl class="footnote brackets">
<dt class="label" id="id30"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Hierzu entsteht derzeit das Dokument
“Compute-Forschungsinfrastrukturen: HPC”.</p>
</dd>
<dt class="label" id="id31"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>BAUER, Jonathan, Dirk von SUCHODOLETZ, Jeannette VOLLMER und
Helena RASCHE, 2019. Game of Templates: Deploying and (re-)using
Virtualized Research Environments in High-Performance and
High-Throughput Computing. In: Michael JANCZYK, Dirk von
SUCHODOLETZ und Bernd WIEBELT (Hrsg.), <em>Proceedings of the 5th
bwHPC Symposium: HPC Activities in Baden-Württemberg.
Freiburg, September 2018</em>. TLP, Tübingen. 2019. S. 245–262</p>
</dd>
<dt class="label" id="id32"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>SUCHODOLETZ, Dirk von, Jonathan BAUER, Oleg ZHARKOV, Susanne
MOCKEN und Björn GRÜNING, 2020. Lessons learned from Virtualized
Research Environments in today’s scientific compute
infrastructures. In: <em>E-Science-Tage 2019: Data to Knowledge</em>.
Heidelberg: heiBOOKS. März 2020. S. 88–81.
ISBN <a class="reference external" href="https://worldcat.org/isbn/978-3-948083-14-4">978-3-948083-14-4</a></p>
</dd>
<dt class="label" id="id33"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>SUCHODOLETZ, Dirk von und Jonathan BAUER, 2020. ViCE – Creating
Uniform Approach to Large-Scale Research Infrastructures. In:
<em>E-Science-Tage 2019: Data to Knowledge</em>. Heidelberg: heiBOOKS.
März 2020. S. 218–222.
ISBN <a class="reference external" href="https://worldcat.org/isbn/978-3-948083-14-4">978-3-948083-14-4</a></p>
</dd>
<dt class="label" id="id34"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>MEIER, Konrad, Björn GRÜNING, Clemens BLANK, Michael JANCZYK und
Dirk von SUCHODOLETZ, 2017. Virtualisierte wissenschaftliche
Forschungsumgebungen und die zukünftige Rolle der Rechenzentren.
In: <em>10. DFN-Forum Kommunikationstechnologien, 30.-31. Mai 2017,
Berlin, Gesellschaft für Informatik eV (GI)</em>. 2017. S. 145–154</p>
</dd>
<dt class="label" id="id35"><span class="brackets"><a class="fn-backref" href="#id6">6</a></span></dt>
<dd><p>MEIER, Konrad, 2017. <em>Infrastrukturkonzepte für virtualisierte
wissenschaftliche Forschungsumgebungen</em>. phdthesis.
Albert-Ludwigs-Universität Freiburg im Breisgau</p>
</dd>
<dt class="label" id="id36"><span class="brackets"><a class="fn-backref" href="#id7">7</a></span></dt>
<dd><p>ESCIENCE TEAM, 2016. <em>Cluster Betrieb: High Performance Computing</em>
[online]. techreport. Rechenzentrum der Universität Freiburg.
Verfügbar unter:
<a class="reference external" href="https://www.rz.uni-freiburg.de/inhalt/dokumente/pdfs/dienstbeschreibung-hpc">https://www.rz.uni-freiburg.de/inhalt/dokumente/pdfs/dienstbeschreibung-hpc</a></p>
</dd>
<dt class="label" id="id37"><span class="brackets"><a class="fn-backref" href="#id8">8</a></span></dt>
<dd><p>Die aktuelle Hardware des bwForClusters NEMO im zentralen Wiki
dokumentiert:
<a class="reference external" href="https://wiki.bwhpc.de/e/BwForCluster_NEMO_Hardware_and_Architecture#Compute_and_Special_Purpose_Nodes">https://wiki.bwhpc.de/e/BwForCluster_NEMO_Hardware_and_Architecture#Compute_and_Special_Purpose_Nodes</a>,
besucht am 19.04.2021.</p>
</dd>
<dt class="label" id="id38"><span class="brackets"><a class="fn-backref" href="#id9">9</a></span></dt>
<dd><p>Der Zugriff ist auf die IPv4-Prefixe des Belwü-Netzes beschränkt:
<a class="reference external" href="https://bgpview.io/asn/553">https://bgpview.io/asn/553</a>, besucht am 16.04.2021.</p>
</dd>
<dt class="label" id="id39"><span class="brackets"><a class="fn-backref" href="#id10">10</a></span></dt>
<dd><p>Webseite Infoblox: <a class="reference external" href="https://www.infoblox.com/">https://www.infoblox.com/</a>, besucht am 20.04.2021.</p>
</dd>
<dt class="label" id="id40"><span class="brackets"><a class="fn-backref" href="#id11">11</a></span></dt>
<dd><p>Derzeit wird CentOS7 als Betriebssystem eingesetzt. Das
Nachfolgecluster wird RHEL8 oder ein binärkompatibles Derivat
einsetzen.</p>
</dd>
<dt class="label" id="id41"><span class="brackets"><a class="fn-backref" href="#id12">12</a></span></dt>
<dd><p>BAUER, Jonathan, Manuel MESSNER, Michael JANCZYK, Dirk von
SUCHODOLETZ, Bernd WIEBELT und Helena RASCHE, 2019. A Sorting Hat
For Clusters: Dynamic Provisioning of Compute Nodes for Colocated
Large Scale Computational Research Infrastructures. In: Michael
JANCZYK, Dirk von SUCHODOLETZ und Bernd WIEBELT (Hrsg.),
<em>Proceedings of the 5th bwHPC Symposium: HPC Activities in
Baden-Württemberg.
Freiburg, September 2018</em>. TLP, Tübingen. 2019. S. 217–229</p>
</dd>
<dt class="label" id="id42"><span class="brackets"><a class="fn-backref" href="#id13">13</a></span></dt>
<dd><p>STORAGE UND VIRTUALISIERUNGSGRUPPE, 2019. <em>Speichersysteme für die
Universität</em> [online]. techreport. Rechenzentrum der Universität
Freiburg. Verfügbar unter:
<a class="reference external" href="https://www.rz.uni-freiburg.de/inhalt/dokumente/pdfs/speichersysteme">https://www.rz.uni-freiburg.de/inhalt/dokumente/pdfs/speichersysteme</a></p>
</dd>
<dt class="label" id="id43"><span class="brackets"><a class="fn-backref" href="#id14">14</a></span></dt>
<dd><p>Webseite zum Parallelspeicher BeeGFS: <a class="reference external" href="https://www.beegfs.io/">https://www.beegfs.io/</a>,
besucht am 20.04.2021.</p>
</dd>
<dt class="label" id="id44"><span class="brackets"><a class="fn-backref" href="#id15">15</a></span></dt>
<dd><p>Die Dokumente zu bwSFS werden derzeit noch erarbeitet. Diese werden
nachgereicht.</p>
</dd>
<dt class="label" id="id45"><span class="brackets"><a class="fn-backref" href="#id16">16</a></span></dt>
<dd><p>Webseite von ATLAS-BFG: <a class="reference external" href="https://www.hpc.uni-freiburg.de/atlas-bfg">https://www.hpc.uni-freiburg.de/atlas-bfg</a>,
besucht am 20.04.2021.</p>
</dd>
<dt class="label" id="id46"><span class="brackets"><a class="fn-backref" href="#id17">17</a></span></dt>
<dd><p>Github-Repo zu Workspaces:
<a class="reference external" href="https://github.com/holgerBerger/hpc-workspace">https://github.com/holgerBerger/hpc-workspace</a>, besucht am 19.04.2021</p>
</dd>
<dt class="label" id="id47"><span class="brackets"><a class="fn-backref" href="#id18">18</a></span></dt>
<dd><p>Wiki-Eintrag zu LACP: <a class="reference external" href="https://de.wikipedia.org/wiki/Link_Aggregation">https://de.wikipedia.org/wiki/Link_Aggregation</a>,
besucht am 19.02.2021.</p>
</dd>
<dt class="label" id="id48"><span class="brackets"><a class="fn-backref" href="#id19">19</a></span></dt>
<dd><p>Eintrag zu Omni-Path: <a class="reference external" href="https://de.wikipedia.org/wiki/Intel_Omni-Path">https://de.wikipedia.org/wiki/Intel_Omni-Path</a>,
besucht am 19.02.2021.</p>
</dd>
<dt class="label" id="id49"><span class="brackets"><a class="fn-backref" href="#id20">20</a></span></dt>
<dd><p>Registrierungsprozedur im Wiki:
<a class="reference external" href="https://wiki.bwhpc.de/e/BwForCluster_User_Access">https://wiki.bwhpc.de/e/BwForCluster_User_Access</a>, besucht am
20.04.2021.</p>
</dd>
<dt class="label" id="id50"><span class="brackets"><a class="fn-backref" href="#id21">21</a></span></dt>
<dd><p>UNIVERSITÄT FREIBURG, 1981. <em>Verwaltungs- und Benutzungsordnung:
(VBO)</em> [online]. techreport. Universität Freiburg. Verfügbar
unter: <a class="reference external" href="https://www.hpc.uni-freiburg.de/content/legalstuff/vbo.pdf">https://www.hpc.uni-freiburg.de/content/legalstuff/vbo.pdf</a></p>
</dd>
<dt class="label" id="id51"><span class="brackets"><a class="fn-backref" href="#id22">22</a></span></dt>
<dd><p>UNIVERSITÄT FREIBURG, 1996. <em>Benutzungsordnung für die vom
Rechenzentrum der Albert-Ludwigs-Universität angebotenen
Netzdienste: (NBO)</em> [online]. techreport. Universität Freiburg.
Verfügbar unter:
<a class="reference external" href="https://www.hpc.uni-freiburg.de/content/legalstuff/nbo.pdf">https://www.hpc.uni-freiburg.de/content/legalstuff/nbo.pdf</a></p>
</dd>
<dt class="label" id="id52"><span class="brackets"><a class="fn-backref" href="#id23">23</a></span></dt>
<dd><p>UNIVERSITÄT FREIBURG, 1996. <em>Netzordnung für das Freiburger
Universitäts Netz: (NO)</em> [online]. techreport. Universität
Freiburg. Verfügbar unter:
<a class="reference external" href="https://www.hpc.uni-freiburg.de/content/legalstuff/no.pdf">https://www.hpc.uni-freiburg.de/content/legalstuff/no.pdf</a></p>
</dd>
<dt class="label" id="id53"><span class="brackets"><a class="fn-backref" href="#id24">24</a></span></dt>
<dd><p>SUCHODOLETZ, Dirk von, Stefan WESNER und Gerhard SCHNEIDER, 2016.
Überlegungen zu laufenden Cluster-Erweiterungen
in bwHPC. In: Dirk von SUCHODOLETZ, Janne Chr. SCHULZ, Jan
LEENDERTSE, Hartmut HOTZEL und Martin WIMMER (Hrsg.), <em>Kooperation
von Rechenzentren: Governance und Steuerung – Organisation,
Rechtsgrundlagen, Politik</em>. De Gruyter. 2016. S. 331–342.
ISBN <a class="reference external" href="https://worldcat.org/isbn/978-3-11-045888-6">978-3-11-045888-6</a></p>
</dd>
<dt class="label" id="id54"><span class="brackets"><a class="fn-backref" href="#id25">25</a></span></dt>
<dd><p>Erklärung des Fairshare-Mechanismus Anhand der Anleitung des
Schedulers Moab:
<a class="reference external" href="http://docs.adaptivecomputing.com/9-1-3/suite/help.htm#topics/moabWorkloadManager/fairness/fairnessoverview.html">http://docs.adaptivecomputing.com/9-1-3/suite/help.htm#topics/moabWorkloadManager/fairness/fairnessoverview.html</a>,
besucht am 20.04.2021.</p>
</dd>
<dt class="label" id="id55"><span class="brackets"><a class="fn-backref" href="#id27">26</a></span></dt>
<dd><p>Zabbix Monitoring-Lösung: <a class="reference external" href="https://www.zabbix.com">https://www.zabbix.com</a>, besucht am
20.04.2021.</p>
</dd>
<dt class="label" id="id56"><span class="brackets"><a class="fn-backref" href="#id28">27</a></span></dt>
<dd><p>SCHULZ, Janne Chr., Dirk von SUCHODOLETZ, Ulrich GEHRING,
Willibald MEYER und Jan LEENDERTSE, 2020.
<em>Maschinensaalbenutzungsordnung des Rechenzentrums der Universität
Freiburg: Richtlinien für das Hosting und Housing von Hardware in
den Räumen desRechenzentrums der Universität Freiburg</em> [online].
techreport. Rechenzentrum der Universität Freiburg. Verfügbar
unter: <a class="reference external" href="https://www.rz.uni-freiburg.de/inhalt/dokumente/pdfs/msbo">https://www.rz.uni-freiburg.de/inhalt/dokumente/pdfs/msbo</a></p>
</dd>
<dt class="label" id="id57"><span class="brackets"><a class="fn-backref" href="#id29">28</a></span></dt>
<dd><p>SUCHODOLETZ, Dirk von, Ulrich GEHRING und Jan LEENDERTSE, 2020.
<em>Machine-Hosting: Bereitstellung von Rackspace in den
Maschinensälen des RZ. externe Version</em> [online]. techreport.
Rechenzentrum der Universität Freiburg. Verfügbar unter:
<a class="reference external" href="https://www.rz.uni-freiburg.de/inhalt/dokumente/pdfs/dienstbeschr-machine-hosting">https://www.rz.uni-freiburg.de/inhalt/dokumente/pdfs/dienstbeschr-machine-hosting</a></p>
</dd>
</dl>
</div>
</div>


              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="index.html" title="previous page">bwForCluster NEMO</a>
    <a class='right-next' id="next-link" href="toms.html" title="next page">Technisch Organisatorische Maßnahmen</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          Durch Michael Janczyk (MJ), Jan Leendertse (JL), Dirk von Suchodoletz (DvS), Bernd Wiebelt (BW)<br/>
        
            &copy; Urheberrechte © 2021, eScience, Rechenzentrum, Albert-Ludwigs-Universität Freiburg.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>